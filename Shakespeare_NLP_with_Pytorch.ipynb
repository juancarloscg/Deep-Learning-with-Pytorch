{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shakespeare - NLP with Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOrvwbFPykP3omPzRB9ix8k"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agri30_lBO52",
        "colab_type": "text"
      },
      "source": [
        "**Text prediction of Shakespeare's literature**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtCOOVlb_WGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKx-bEz_AT6c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92540934-2257-4725-ee72-db23e91358d7"
      },
      "source": [
        "#Mounting drive to access Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with open('/content/drive/My Drive/Colab Notebooks/shakespeare.txt','r',encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iMnIAkiArBc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3ab8729-63de-4e40-a9e6-c8e8eaa9d69d"
      },
      "source": [
        "type(text)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnPx3WZtBHF2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "444c687c-a7e6-423c-8d5f-dc79318efcb5"
      },
      "source": [
        "text[:1000]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\\n\\n\\n                     2\\n  When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty's field,\\n  Thy youth's proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:  \\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say within thine own deep su\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwSw0QmmBI1z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "1bb8b116-5282-4f92-da2b-463a46e436e0"
      },
      "source": [
        "print(text[:1000])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bud buriest thy content,\n",
            "  And tender churl mak'st waste in niggarding:\n",
            "    Pity the world, or else this glutton be,\n",
            "    To eat the world's due, by the grave and thee.\n",
            "\n",
            "\n",
            "                     2\n",
            "  When forty winters shall besiege thy brow,\n",
            "  And dig deep trenches in thy beauty's field,\n",
            "  Thy youth's proud livery so gazed on now,\n",
            "  Will be a tattered weed of small worth held:  \n",
            "  Then being asked, where all thy beauty lies,\n",
            "  Where all the treasure of thy lusty days;\n",
            "  To say within thine own deep su\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbhfxqldBL7E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db7b9cd9-450e-4280-eb6f-0e421d8a74db"
      },
      "source": [
        "len(text)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5445609"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GL2yBmWBYmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_characters = set(text)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I9raKE4B-C_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e3b8abd-da66-48ec-a93c-f598c50af05b"
      },
      "source": [
        "all_characters"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '<',\n",
              " '>',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " '[',\n",
              " ']',\n",
              " '_',\n",
              " '`',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '|',\n",
              " '}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muR81g3DB_Y5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab0efbc0-fac1-496d-b213-ed662c6e1edd"
      },
      "source": [
        "len(all_characters)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNBPQ4nnCEhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = dict(enumerate(all_characters))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4LCBcmHCNva",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e399a11c-c809-44c3-b82e-f1da3a9d01ae"
      },
      "source": [
        "for pair in enumerate(all_characters): print(pair)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, '[')\n",
            "(1, 'h')\n",
            "(2, 'f')\n",
            "(3, ':')\n",
            "(4, 'W')\n",
            "(5, '-')\n",
            "(6, '`')\n",
            "(7, 'w')\n",
            "(8, ',')\n",
            "(9, '?')\n",
            "(10, 'q')\n",
            "(11, '7')\n",
            "(12, '>')\n",
            "(13, '4')\n",
            "(14, \"'\")\n",
            "(15, 'j')\n",
            "(16, 'M')\n",
            "(17, '3')\n",
            "(18, '0')\n",
            "(19, 'z')\n",
            "(20, 'E')\n",
            "(21, 'O')\n",
            "(22, 'G')\n",
            "(23, 'Q')\n",
            "(24, '&')\n",
            "(25, 'R')\n",
            "(26, 'x')\n",
            "(27, 'l')\n",
            "(28, '_')\n",
            "(29, ' ')\n",
            "(30, 'b')\n",
            "(31, '6')\n",
            "(32, '}')\n",
            "(33, 'I')\n",
            "(34, 'C')\n",
            "(35, '\\n')\n",
            "(36, 'd')\n",
            "(37, 's')\n",
            "(38, 'n')\n",
            "(39, 'B')\n",
            "(40, 'c')\n",
            "(41, ';')\n",
            "(42, '5')\n",
            "(43, ')')\n",
            "(44, '!')\n",
            "(45, 'S')\n",
            "(46, 'v')\n",
            "(47, '|')\n",
            "(48, 'm')\n",
            "(49, 'e')\n",
            "(50, 'X')\n",
            "(51, '\"')\n",
            "(52, '2')\n",
            "(53, '8')\n",
            "(54, 'J')\n",
            "(55, 'P')\n",
            "(56, 'K')\n",
            "(57, 'u')\n",
            "(58, 'p')\n",
            "(59, '.')\n",
            "(60, 'L')\n",
            "(61, 'Y')\n",
            "(62, 'Z')\n",
            "(63, 'a')\n",
            "(64, 'N')\n",
            "(65, 'o')\n",
            "(66, 'F')\n",
            "(67, '(')\n",
            "(68, 'H')\n",
            "(69, 'A')\n",
            "(70, '<')\n",
            "(71, 'D')\n",
            "(72, '9')\n",
            "(73, ']')\n",
            "(74, 'k')\n",
            "(75, 'i')\n",
            "(76, 't')\n",
            "(77, 'y')\n",
            "(78, 'T')\n",
            "(79, '1')\n",
            "(80, 'g')\n",
            "(81, 'r')\n",
            "(82, 'U')\n",
            "(83, 'V')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rBHjezBCTAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = {char: ind for ind, char in decoder.items()}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P1-v_fcCeBf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "40256ea8-8a7d-482f-ce0c-18c09a25780f"
      },
      "source": [
        "encoder"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 35,\n",
              " ' ': 29,\n",
              " '!': 44,\n",
              " '\"': 51,\n",
              " '&': 24,\n",
              " \"'\": 14,\n",
              " '(': 67,\n",
              " ')': 43,\n",
              " ',': 8,\n",
              " '-': 5,\n",
              " '.': 59,\n",
              " '0': 18,\n",
              " '1': 79,\n",
              " '2': 52,\n",
              " '3': 17,\n",
              " '4': 13,\n",
              " '5': 42,\n",
              " '6': 31,\n",
              " '7': 11,\n",
              " '8': 53,\n",
              " '9': 72,\n",
              " ':': 3,\n",
              " ';': 41,\n",
              " '<': 70,\n",
              " '>': 12,\n",
              " '?': 9,\n",
              " 'A': 69,\n",
              " 'B': 39,\n",
              " 'C': 34,\n",
              " 'D': 71,\n",
              " 'E': 20,\n",
              " 'F': 66,\n",
              " 'G': 22,\n",
              " 'H': 68,\n",
              " 'I': 33,\n",
              " 'J': 54,\n",
              " 'K': 56,\n",
              " 'L': 60,\n",
              " 'M': 16,\n",
              " 'N': 64,\n",
              " 'O': 21,\n",
              " 'P': 55,\n",
              " 'Q': 23,\n",
              " 'R': 25,\n",
              " 'S': 45,\n",
              " 'T': 78,\n",
              " 'U': 82,\n",
              " 'V': 83,\n",
              " 'W': 4,\n",
              " 'X': 50,\n",
              " 'Y': 61,\n",
              " 'Z': 62,\n",
              " '[': 0,\n",
              " ']': 73,\n",
              " '_': 28,\n",
              " '`': 6,\n",
              " 'a': 63,\n",
              " 'b': 30,\n",
              " 'c': 40,\n",
              " 'd': 36,\n",
              " 'e': 49,\n",
              " 'f': 2,\n",
              " 'g': 80,\n",
              " 'h': 1,\n",
              " 'i': 75,\n",
              " 'j': 15,\n",
              " 'k': 74,\n",
              " 'l': 27,\n",
              " 'm': 48,\n",
              " 'n': 38,\n",
              " 'o': 65,\n",
              " 'p': 58,\n",
              " 'q': 10,\n",
              " 'r': 81,\n",
              " 's': 37,\n",
              " 't': 76,\n",
              " 'u': 57,\n",
              " 'v': 46,\n",
              " 'w': 7,\n",
              " 'x': 26,\n",
              " 'y': 77,\n",
              " 'z': 19,\n",
              " '|': 47,\n",
              " '}': 32}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JC-CqZMCe0l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "11cfb04c-a577-400f-b58d-7db0407041a8"
      },
      "source": [
        "decoder.items()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([(0, '['), (1, 'h'), (2, 'f'), (3, ':'), (4, 'W'), (5, '-'), (6, '`'), (7, 'w'), (8, ','), (9, '?'), (10, 'q'), (11, '7'), (12, '>'), (13, '4'), (14, \"'\"), (15, 'j'), (16, 'M'), (17, '3'), (18, '0'), (19, 'z'), (20, 'E'), (21, 'O'), (22, 'G'), (23, 'Q'), (24, '&'), (25, 'R'), (26, 'x'), (27, 'l'), (28, '_'), (29, ' '), (30, 'b'), (31, '6'), (32, '}'), (33, 'I'), (34, 'C'), (35, '\\n'), (36, 'd'), (37, 's'), (38, 'n'), (39, 'B'), (40, 'c'), (41, ';'), (42, '5'), (43, ')'), (44, '!'), (45, 'S'), (46, 'v'), (47, '|'), (48, 'm'), (49, 'e'), (50, 'X'), (51, '\"'), (52, '2'), (53, '8'), (54, 'J'), (55, 'P'), (56, 'K'), (57, 'u'), (58, 'p'), (59, '.'), (60, 'L'), (61, 'Y'), (62, 'Z'), (63, 'a'), (64, 'N'), (65, 'o'), (66, 'F'), (67, '('), (68, 'H'), (69, 'A'), (70, '<'), (71, 'D'), (72, '9'), (73, ']'), (74, 'k'), (75, 'i'), (76, 't'), (77, 'y'), (78, 'T'), (79, '1'), (80, 'g'), (81, 'r'), (82, 'U'), (83, 'V')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5ctAxsqCmn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_text = np.array([encoder[char] for char in text])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw83NTU1Czv9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "ed3eda7e-90af-4188-86af-7716be209226"
      },
      "source": [
        "encoded_text[:500]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([35, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
              "       29, 29, 29, 29, 29, 79, 35, 29, 29, 66, 81, 65, 48, 29,  2, 63, 75,\n",
              "       81, 49, 37, 76, 29, 40, 81, 49, 63, 76, 57, 81, 49, 37, 29,  7, 49,\n",
              "       29, 36, 49, 37, 75, 81, 49, 29, 75, 38, 40, 81, 49, 63, 37, 49,  8,\n",
              "       35, 29, 29, 78,  1, 63, 76, 29, 76,  1, 49, 81, 49, 30, 77, 29, 30,\n",
              "       49, 63, 57, 76, 77, 14, 37, 29, 81, 65, 37, 49, 29, 48, 75, 80,  1,\n",
              "       76, 29, 38, 49, 46, 49, 81, 29, 36, 75, 49,  8, 35, 29, 29, 39, 57,\n",
              "       76, 29, 63, 37, 29, 76,  1, 49, 29, 81, 75, 58, 49, 81, 29, 37,  1,\n",
              "       65, 57, 27, 36, 29, 30, 77, 29, 76, 75, 48, 49, 29, 36, 49, 40, 49,\n",
              "       63, 37, 49,  8, 35, 29, 29, 68, 75, 37, 29, 76, 49, 38, 36, 49, 81,\n",
              "       29,  1, 49, 75, 81, 29, 48, 75, 80,  1, 76, 29, 30, 49, 63, 81, 29,\n",
              "        1, 75, 37, 29, 48, 49, 48, 65, 81, 77,  3, 35, 29, 29, 39, 57, 76,\n",
              "       29, 76,  1, 65, 57, 29, 40, 65, 38, 76, 81, 63, 40, 76, 49, 36, 29,\n",
              "       76, 65, 29, 76,  1, 75, 38, 49, 29, 65,  7, 38, 29, 30, 81, 75, 80,\n",
              "        1, 76, 29, 49, 77, 49, 37,  8, 35, 29, 29, 66, 49, 49, 36, 14, 37,\n",
              "       76, 29, 76,  1, 77, 29, 27, 75, 80,  1, 76, 14, 37, 29,  2, 27, 63,\n",
              "       48, 49, 29,  7, 75, 76,  1, 29, 37, 49, 27,  2,  5, 37, 57, 30, 37,\n",
              "       76, 63, 38, 76, 75, 63, 27, 29,  2, 57, 49, 27,  8, 35, 29, 29, 16,\n",
              "       63, 74, 75, 38, 80, 29, 63, 29,  2, 63, 48, 75, 38, 49, 29,  7,  1,\n",
              "       49, 81, 49, 29, 63, 30, 57, 38, 36, 63, 38, 40, 49, 29, 27, 75, 49,\n",
              "       37,  8, 35, 29, 29, 78,  1, 77, 29, 37, 49, 27,  2, 29, 76,  1, 77,\n",
              "       29,  2, 65, 49,  8, 29, 76, 65, 29, 76,  1, 77, 29, 37,  7, 49, 49,\n",
              "       76, 29, 37, 49, 27,  2, 29, 76, 65, 65, 29, 40, 81, 57, 49, 27,  3,\n",
              "       35, 29, 29, 78,  1, 65, 57, 29, 76,  1, 63, 76, 29, 63, 81, 76, 29,\n",
              "       38, 65,  7, 29, 76,  1, 49, 29,  7, 65, 81, 27, 36, 14, 37, 29,  2,\n",
              "       81, 49, 37,  1, 29, 65, 81, 38, 63, 48, 49, 38, 76,  8, 35, 29, 29,\n",
              "       69, 38, 36, 29, 65, 38, 27, 77, 29,  1, 49, 81, 63, 27, 36, 29, 76,\n",
              "       65, 29, 76,  1, 49, 29, 80, 63, 57, 36, 77, 29, 37, 58, 81, 75, 38,\n",
              "       80,  8, 35, 29, 29,  4, 75, 76,  1, 75, 38, 29, 76,  1, 75, 38, 49,\n",
              "       29, 65,  7, 38, 29, 30, 57])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTZt7Yh9C2_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7bae731d-d94a-4934-86fc-c0a7894ff5ba"
      },
      "source": [
        "decoder[27]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'l'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAXiac0vDC72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encoder(encoded_text,num_uni_chars):\n",
        "\n",
        "  one_hot = np.zeros((encoded_text.size,num_uni_chars))\n",
        "  one_hot = one_hot.astype(np.float32)\n",
        "  one_hot[np.arange(one_hot.shape[0]),encoded_text.flatten()] = 1.0\n",
        "  one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
        "  return one_hot"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtCm01c9GwEf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "19ffcbec-d275-4bff-ba92-c71cb7a69ebf"
      },
      "source": [
        "example_text = np.arange(10)\n",
        "example_text"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HytRs8mMIYQa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "747b7bb9-7bd1-4174-aacf-8ee18d97886c"
      },
      "source": [
        "example_text.reshape((5,-1))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1],\n",
              "       [2, 3],\n",
              "       [4, 5],\n",
              "       [6, 7],\n",
              "       [8, 9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_DwNGQtIceh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batches(encoded_text,samp_per_batch=10,seq_len=50):\n",
        "  #X: encoded text of length seq_len\n",
        "  #Y: encoded text shifted by one\n",
        "\n",
        "  #How many chars per batch?\n",
        "  char_per_batch = samp_per_batch * seq_len\n",
        "\n",
        "  #How many batches in the given encoded text\n",
        "  num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
        "\n",
        "  #Cut off the end of teh encoded text\n",
        "  encoded_text = encoded_text[:num_batches_avail*char_per_batch]\n",
        "\n",
        "  encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
        "\n",
        "  for n in range(0, encoded_text.shape[1], seq_len):\n",
        "    x = encoded_text[:,n:n+seq_len]\n",
        "    y = np.zeros_like(x)\n",
        "\n",
        "    try:\n",
        "\n",
        "      y[:,:-1]=x[:,1:]\n",
        "      y[:,-1]=encoded_text[:,n+seq_len]\n",
        "\n",
        "    except:\n",
        "      y[:,:-1]= x[:,1:]\n",
        "      y[:,-1]=encoded_text[:,0]\n",
        "\n",
        "    yield x,y"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld2Y9uY9NAdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharModel(nn.Module):\n",
        "  def __init__(self, all_chars, num_hidden=256, num_layers=4, drop_prob=0.5,use_gpu=False):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.drop_prob=drop_prob\n",
        "    self.num_layers=num_layers\n",
        "    self.num_hidden=num_hidden\n",
        "    self.use_gpu = use_gpu\n",
        "\n",
        "    self.all_chars=all_characters\n",
        "    self.decoder = dict(enumerate(all_chars))\n",
        "    self.encoder = {char:ind for ind, char in decoder.items()}\n",
        "\n",
        "    self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
        "\n",
        "  def forward(self,x,hidden):\n",
        "    lstm_output, hidden = self.lstm(x, hidden)\n",
        "    drop_output = self.dropout(lstm_output)\n",
        "    drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
        "    final_out = self.fc_linear(drop_output)\n",
        "    return final_out, hidden\n",
        "  \n",
        "  def hidden_state(self, batch_size):\n",
        "\n",
        "      if self.use_gpu:\n",
        "\n",
        "        hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda(),torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda())\n",
        "      \n",
        "      else:\n",
        "\n",
        "        hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden),torch.zeros(self.num_layers, batch_size, self.num_hidden))\n",
        "\n",
        "      return hidden"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ab55tG6FLUAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CharModel(all_chars = all_characters, num_hidden = 512, num_layers = 3,drop_prob=0.5,use_gpu=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yovH8-ODLlOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_param = []\n",
        "\n",
        "for p in model.parameters():\n",
        "    total_param.append(int(p.numel()))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8DGyUL3L4WO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d43c2405-06d7-4148-9d3c-c0e19e68860d"
      },
      "source": [
        "sum(total_param)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5470292"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u17xYtf_MGPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YOOR_w-Mmeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_percent = 0.1\n",
        "train_ind = int(len(encoded_text)*train_percent)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CCk5kIeN0xi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = encoded_text[:train_ind]\n",
        "val_data = encoded_text[train_ind:]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OGtg95tOqvr",
        "colab_type": "text"
      },
      "source": [
        "**Training the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b42owKxjOTgr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 60\n",
        "batch_size = 100\n",
        "\n",
        "seq_len = 100\n",
        "tracker = 0\n",
        "\n",
        "num_char = max(encoded_text)+1"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N7VbimeO6sR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6f541e67-1468-45e8-ae9e-755bb808df24"
      },
      "source": [
        "model.train()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharModel(\n",
              "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyOJWZMfO__r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "145bb278-9bd2-43ee-a9a8-bdc378faa8ed"
      },
      "source": [
        "if model.use_gpu: model.cuda()\n",
        "\n",
        "for i in range(epochs):\n",
        "  hidden = model.hidden_state(batch_size)\n",
        "\n",
        "  for x,y in generate_batches(train_data, batch_size,seq_len):\n",
        "    tracker +=1\n",
        "    x = one_hot_encoder(x, num_char)\n",
        "\n",
        "    inputs = torch.from_numpy(x)\n",
        "    targets = torch.from_numpy(y)\n",
        "\n",
        "    if model.use_gpu:\n",
        "      inputs = inputs.cuda()\n",
        "      targets = targets.cuda()\n",
        "\n",
        "    hidden = tuple([state.data for state in hidden])\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    lstm_output, hidden = model.forward(inputs, hidden)\n",
        "    loss = criterion(lstm_output, targets.view(batch_size*seq_len).long())\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm(model.parameters(),max_norm=5)\n",
        "    optimizer.step()\n",
        "\n",
        "    if tracker % 25 ==0:\n",
        "\n",
        "      val_hidden = model.hidden_state(batch_size)\n",
        "      val_losses = []\n",
        "  \n",
        "  model.eval()\n",
        "\n",
        "  for x,y in generate_batches(val_data, batch_size,seq_len):\n",
        "        x = one_hot_encoder(x, num_char)\n",
        "        inputs = torch.from_numpy(x)\n",
        "        targets = torch.from_numpy(y)\n",
        "\n",
        "        if model.use_gpu:\n",
        "             inputs = inputs.cuda()\n",
        "             targets = targets.cuda()\n",
        "\n",
        "        val_hidden = tuple([state.data for state in val_hidden])\n",
        "        lstm_output, val_hidden = model.forward(inputs, val_hidden)\n",
        "        val_loss = criterion(lstm_output, targets.view(batch_size*seq_len).long())\n",
        "\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  print(f'Epoch: {i} Step: {tracker} Val Loss:{val_loss.item()}')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Step: 54 Val Loss:3.1829493045806885\n",
            "Epoch: 1 Step: 108 Val Loss:3.1631569862365723\n",
            "Epoch: 2 Step: 162 Val Loss:2.948288917541504\n",
            "Epoch: 3 Step: 216 Val Loss:2.7142174243927\n",
            "Epoch: 4 Step: 270 Val Loss:2.5716471672058105\n",
            "Epoch: 5 Step: 324 Val Loss:2.3568639755249023\n",
            "Epoch: 6 Step: 378 Val Loss:2.210324764251709\n",
            "Epoch: 7 Step: 432 Val Loss:2.132627010345459\n",
            "Epoch: 8 Step: 486 Val Loss:2.062723159790039\n",
            "Epoch: 9 Step: 540 Val Loss:2.020033121109009\n",
            "Epoch: 10 Step: 594 Val Loss:1.9783984422683716\n",
            "Epoch: 11 Step: 648 Val Loss:1.942792534828186\n",
            "Epoch: 12 Step: 702 Val Loss:1.91409432888031\n",
            "Epoch: 13 Step: 756 Val Loss:1.8880290985107422\n",
            "Epoch: 14 Step: 810 Val Loss:1.8618855476379395\n",
            "Epoch: 15 Step: 864 Val Loss:1.838640809059143\n",
            "Epoch: 16 Step: 918 Val Loss:1.82328200340271\n",
            "Epoch: 17 Step: 972 Val Loss:1.8105007410049438\n",
            "Epoch: 18 Step: 1026 Val Loss:1.7880192995071411\n",
            "Epoch: 19 Step: 1080 Val Loss:1.7787673473358154\n",
            "Epoch: 20 Step: 1134 Val Loss:1.7658336162567139\n",
            "Epoch: 21 Step: 1188 Val Loss:1.7485361099243164\n",
            "Epoch: 22 Step: 1242 Val Loss:1.744853138923645\n",
            "Epoch: 23 Step: 1296 Val Loss:1.7339164018630981\n",
            "Epoch: 24 Step: 1350 Val Loss:1.7220728397369385\n",
            "Epoch: 25 Step: 1404 Val Loss:1.7121890783309937\n",
            "Epoch: 26 Step: 1458 Val Loss:1.7060519456863403\n",
            "Epoch: 27 Step: 1512 Val Loss:1.6972912549972534\n",
            "Epoch: 28 Step: 1566 Val Loss:1.6842405796051025\n",
            "Epoch: 29 Step: 1620 Val Loss:1.6852366924285889\n",
            "Epoch: 30 Step: 1674 Val Loss:1.6722136735916138\n",
            "Epoch: 31 Step: 1728 Val Loss:1.6675223112106323\n",
            "Epoch: 32 Step: 1782 Val Loss:1.6615113019943237\n",
            "Epoch: 33 Step: 1836 Val Loss:1.6693201065063477\n",
            "Epoch: 34 Step: 1890 Val Loss:1.6619462966918945\n",
            "Epoch: 35 Step: 1944 Val Loss:1.6576324701309204\n",
            "Epoch: 36 Step: 1998 Val Loss:1.660276174545288\n",
            "Epoch: 37 Step: 2052 Val Loss:1.6566030979156494\n",
            "Epoch: 38 Step: 2106 Val Loss:1.6573785543441772\n",
            "Epoch: 39 Step: 2160 Val Loss:1.6554324626922607\n",
            "Epoch: 40 Step: 2214 Val Loss:1.6534680128097534\n",
            "Epoch: 41 Step: 2268 Val Loss:1.651557207107544\n",
            "Epoch: 42 Step: 2322 Val Loss:1.6506544351577759\n",
            "Epoch: 43 Step: 2376 Val Loss:1.6475787162780762\n",
            "Epoch: 44 Step: 2430 Val Loss:1.6467796564102173\n",
            "Epoch: 45 Step: 2484 Val Loss:1.647536277770996\n",
            "Epoch: 46 Step: 2538 Val Loss:1.653747797012329\n",
            "Epoch: 47 Step: 2592 Val Loss:1.6541259288787842\n",
            "Epoch: 48 Step: 2646 Val Loss:1.6494271755218506\n",
            "Epoch: 49 Step: 2700 Val Loss:1.6463611125946045\n",
            "Epoch: 50 Step: 2754 Val Loss:1.6486228704452515\n",
            "Epoch: 51 Step: 2808 Val Loss:1.6512845754623413\n",
            "Epoch: 52 Step: 2862 Val Loss:1.6493576765060425\n",
            "Epoch: 53 Step: 2916 Val Loss:1.6477938890457153\n",
            "Epoch: 54 Step: 2970 Val Loss:1.648439645767212\n",
            "Epoch: 55 Step: 3024 Val Loss:1.6448869705200195\n",
            "Epoch: 56 Step: 3078 Val Loss:1.6544697284698486\n",
            "Epoch: 57 Step: 3132 Val Loss:1.6596391201019287\n",
            "Epoch: 58 Step: 3186 Val Loss:1.650552749633789\n",
            "Epoch: 59 Step: 3240 Val Loss:1.6536295413970947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhDDUSin1Sv3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = 'example.net'\n",
        "torch.save(model.state_dict(),model_name)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdQpoGuQ1mNu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8836fda0-859e-4159-c9ea-54a07caccfeb"
      },
      "source": [
        "model = CharModel(all_chars=all_characters, num_hidden=512, num_layers=3,drop_prob=0.5,use_gpu=True)\n",
        "model.load_state_dict(torch.load(model_name))\n",
        "\n",
        "model.eval()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharModel(\n",
              "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoxl8PUW2HcZ",
        "colab_type": "text"
      },
      "source": [
        "**Generating Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41Y2lkYv2KRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_next_char(model,char, hidden=None, k=1):\n",
        "\n",
        "  encoded_text=model.encoder[char]\n",
        "  encoded_text = np.array([[encoded_text]])\n",
        "  encoded_text=one_hot_encoder(encoded_text, len(model.all_chars))\n",
        "  inputs = torch.from_numpy(encoded_text)\n",
        "\n",
        "  if (model.use_gpu):\n",
        "    inputs = inputs.cuda()\n",
        "  \n",
        "  hidden = tuple([state.data for state in hidden])\n",
        "  lstm_out, hidden = model(inputs, hidden)\n",
        "  probs = F.softmax(lstm_out, dim=1).data\n",
        "\n",
        "  if (model.use_gpu):probs = probs.cpu()\n",
        "\n",
        "  probs, index_positions = probs.topk(k)\n",
        "  index_positions = index_positions.numpy().squeeze()\n",
        "\n",
        "  probs = probs.numpy().flatten()\n",
        "  probs = probs/probs.sum()\n",
        "  char = np.random.choice(index_positions, p=probs)\n",
        "\n",
        "  return model.decoder[char], hidden"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDpkPEqq6hOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, size, seed='The',k=1):\n",
        "\n",
        "  if(model.use_gpu):\n",
        "    model.cuda()\n",
        "  else:\n",
        "    model.cpu()\n",
        "\n",
        "  model.eval()\n",
        "  output_chars = [c for c in seed]\n",
        "  hidden = model.hidden_state(1)\n",
        "\n",
        "\n",
        "  for char in seed:\n",
        "      char, hidden = predict_next_char(model, char, hidden, k=k)\n",
        "  \n",
        "  output_chars.append(char)\n",
        "\n",
        "  for i in range(size):\n",
        "    char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
        "    output_chars.append(char)\n",
        "  \n",
        "  return ''.join(output_chars)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzrZdMRa7nHM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "35c1f1d9-aa41-44b6-e07c-405040e42223"
      },
      "source": [
        "print(generate_text(model,1000, seed='The',k=3))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The strong parts\n",
            "    And things to that which now between his strong,\n",
            "    And shall the soully works of thee, and to have\n",
            "    The best of heaven's poor thoughts. The suck and mother,\n",
            "    Whilst thou that true and strong that should not buy\n",
            "    The world shall be to hear her. If this thither\n",
            "    I will speak, being breeds and that he shall\n",
            "    Take that we speak thee. The men as senten stands\n",
            "    We will net seek him to him a man from the stattered,\n",
            "    And by their state out of the wind, and to thy speed,\n",
            "    That there were strong together. Therefore do not see,\n",
            "    And that he see himself and man.\n",
            "  CAESAR. That will not love me all the beauty,\n",
            "    That should bring him to my brain, and to her,\n",
            "    And to the sender of his braveres,\n",
            "    To spoke him by a propertily.\n",
            "  CLEOPATRA. Is it not so so should hear and means to thee;\n",
            "    I will not see thee to this, and they say this\n",
            "    The man's show and treasors. I will be bonged\n",
            "    The bosom when I send mine own bound of my\n",
            "    To this words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJIl_gsEHNc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}